#!/bin/bash
#SBATCH --job-name=amyloid_effv2_all
#SBATCH --output=/fslustre/qhs/ext_chen_yuheng_mayo_edu/script/out/amyloid_effnet_v2_all_%A_%a.log
#SBATCH --error=/fslustre/qhs/ext_chen_yuheng_mayo_edu/script/out/amyloid_effnet_v2_all_%A_%a.err
#SBATCH --partition=gpu-n16-60g-1x-tesla-t4
#SBATCH --gres=gpu:1
#SBATCH --mem=50G
#SBATCH --nodes=1
#SBATCH --tasks-per-node=1
#SBATCH --cpus-per-task=8
#SBATCH --time=99:00:00

# ============================================================================
# EfficientNet-B4 v2 Amyloid Segmentation Pipeline
# ============================================================================
# Model: EfficientNet-B4 UNet (epoch 191, val_loss=0.10)
# Config: Tversky α=0.6, β=0.4 | Focal + Tversky Loss
# Performance: Dice=91.49%, IoU=84.32%
# ============================================================================

# --- Script Configuration ---
WSI_DIR_ROOT="/fslustre/qhs/ext_chen_yuheng_mayo_edu/RO1_GCP"
DIR_LIST_FILE="/fslustre/qhs/ext_chen_yuheng_mayo_edu/script/out/amyloid_effnet_v2_dir_list.txt"
FAILED_LIST_FILE="/fslustre/qhs/ext_chen_yuheng_mayo_edu/script/out/amyloid_effnet_v2_failed_slides.txt"
SUCCESS_LIST_FILE="/fslustre/qhs/ext_chen_yuheng_mayo_edu/script/out/amyloid_effnet_v2_success_slides.txt"
CONFIG_FILE="/fslustre/qhs/ext_chen_yuheng_mayo_edu/rhizonet/data/setup_files/setup-predict_Amyloid_efficientnet_v2.json"

# This block runs only when you first execute the script (not in the array tasks)
if [ -z "$SLURM_ARRAY_TASK_ID" ]; then
    echo "============================================================"
    echo "EfficientNet-B4 v2 Amyloid Prediction - Job Preparation"
    echo "============================================================"
    echo "Searching for Amyloid slide directories in: $WSI_DIR_ROOT"
    echo ""
    
    # Generate the list of slide directories to process
    echo "Generating directory list..."
    
    # Find all _files directories under RO1_Amyloid that contain heatmap folder
    # Search in both Pipeline and Pipeline2
    find "$WSI_DIR_ROOT/Pipeline/RO1_Amyloid" "$WSI_DIR_ROOT/Pipeline2/RO1_Amyloid" \
        -type d -name "*_files" 2>/dev/null | \
        while read dir; do
            if [ -d "$dir/heatmap" ]; then
                echo "$dir"
            fi
        done | sort -u > "$DIR_LIST_FILE"
    
    # Count the number of directories
    NUM_DIRS=$(wc -l < "$DIR_LIST_FILE")
    echo "Found $NUM_DIRS slide directories ready for prediction."
    echo ""
    
    # Initialize tracking files
    echo "# Failed slides - EfficientNet v2 Amyloid Prediction" > "$FAILED_LIST_FILE"
    echo "# Generated: $(date)" >> "$FAILED_LIST_FILE"
    echo "" >> "$FAILED_LIST_FILE"
    
    echo "# Successful slides - EfficientNet v2 Amyloid Prediction" > "$SUCCESS_LIST_FILE"
    echo "# Generated: $(date)" >> "$SUCCESS_LIST_FILE"
    echo "" >> "$SUCCESS_LIST_FILE"

    # Clean old predictions and heatmaps before running new model
    echo "Cleaning old predictions (TAU_seg_tiles) and heatmaps (hm_map_*)..."
    CLEANED_COUNT=0
    while IFS= read -r dir; do
        # Remove old TAU_seg_tiles (old model predictions)
        if [ -d "$dir/heatmap/TAU_seg_tiles" ]; then
            rm -rf "$dir/heatmap/TAU_seg_tiles"
            ((CLEANED_COUNT++))
        fi
        # Remove old heatmaps (will be regenerated by pipeline2)
        find "$dir/heatmap" -maxdepth 1 -type d -name "hm_map_*" -exec rm -rf {} \; 2>/dev/null
    done < "$DIR_LIST_FILE"
    echo "Cleaned $CLEANED_COUNT slides with old predictions."
    echo ""

    # Submit this script as a job array
    if [ "$NUM_DIRS" -gt 0 ]; then
        echo "Submitting job array for 1 to $NUM_DIRS tasks."
        echo "Monitor with: squeue -u \$USER"
        echo ""
        echo "After completion, check:"
        echo "  - Failed slides: $FAILED_LIST_FILE"
        echo "  - Success slides: $SUCCESS_LIST_FILE"
        echo ""
        sbatch --array=1-$NUM_DIRS "$0"
    else
        echo "No slide directories found ready for prediction."
        echo "Make sure slides have been tiled (contain 'heatmap' folder)."
    fi
    exit 0
fi

# --- This part of the script is executed by each SLURM array task ---

echo "============================================================"
echo "SLURM Array Task $SLURM_ARRAY_TASK_ID - Starting"
echo "============================================================"

# Get the specific directory path for this task from the list
DIR_TO_PROCESS=$(sed -n "${SLURM_ARRAY_TASK_ID}p" "$DIR_LIST_FILE")
SLIDE_NAME=$(basename "$DIR_TO_PROCESS")

echo "Task $SLURM_ARRAY_TASK_ID: Processing $SLIDE_NAME"
echo "Full path: $DIR_TO_PROCESS"
echo ""

# Initialize conda and set environment
source ~/miniconda3/etc/profile.d/conda.sh
conda activate rhizonet
export WANDB_MODE=disabled

SYSTEM_CUDA_PATH=/usr/local/biotools/cuda/12.1
export LD_LIBRARY_PATH=${CONDA_PREFIX}/lib:${SYSTEM_CUDA_PATH}/lib64
export PATH=${SYSTEM_CUDA_PATH}/bin:${PATH}

# Run the prediction
PREDICTION_SUCCESS=0

if [ -d "$DIR_TO_PROCESS" ]; then
    echo "--- Environment ---"
    nvidia-smi
    echo "-------------------"
    echo ""
    echo "Starting prediction at $(date)"
    
    # Run prediction with EfficientNet-specific script
    python /fslustre/qhs/ext_chen_yuheng_mayo_edu/rhizonet/rhizonet/predict_iron_effnet.py \
        --config_file "$CONFIG_FILE" \
        --wsi_dir "$DIR_TO_PROCESS"
    
    PRED_EXIT_CODE=$?
    
    if [ $PRED_EXIT_CODE -eq 0 ]; then
        echo "Prediction completed successfully at $(date)"
        PREDICTION_SUCCESS=1
    else
        echo "ERROR: Prediction failed with exit code $PRED_EXIT_CODE"
    fi
else
    echo "ERROR: Directory '$DIR_TO_PROCESS' not found!"
fi

# Log success or failure
if [ $PREDICTION_SUCCESS -eq 1 ]; then
    echo "$DIR_TO_PROCESS" >> "$SUCCESS_LIST_FILE"
    echo "✓ Logged to success list"
else
    echo "$DIR_TO_PROCESS | Task $SLURM_ARRAY_TASK_ID | $(date)" >> "$FAILED_LIST_FILE"
    echo "✗ Logged to failed list"
fi

echo ""
echo "============================================================"
echo "SLURM Array Task $SLURM_ARRAY_TASK_ID - Complete"
echo "============================================================"
